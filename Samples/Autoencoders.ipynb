{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baby's first Autoencoder\n",
    "\n",
    "We'll start simple, with a single fully-connected neural layer as encoder and as decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a separate encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also make a decoder model here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our autoencoder to reconstruct MNIST digits.\n",
    "\n",
    "First, we'll configure our model to use a per-pixel binary crossentropy loss, and the Adadelta optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare our input data. We're using MNIST digits, and we're discarding the labels (since we're only interested in encoding/decoding the input images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll normalise all the values between 0 and 1, and we'll flatten the 28x28 images into vectors of size 784 (by concatenation of the rows.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print (x_train.shape)\n",
    "print (x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train the autoencoder for 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.3533 - val_loss: 0.2706\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.2632 - val_loss: 0.2518\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.2413 - val_loss: 0.2281\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.2194 - val_loss: 0.2087\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.2036 - val_loss: 0.1962\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1931 - val_loss: 0.1873\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1852 - val_loss: 0.1802\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1786 - val_loss: 0.1743\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1730 - val_loss: 0.1692\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1682 - val_loss: 0.1646\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1639 - val_loss: 0.1606\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1600 - val_loss: 0.1569\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1565 - val_loss: 0.1535\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1532 - val_loss: 0.1503\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1502 - val_loss: 0.1474\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1473 - val_loss: 0.1446\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1445 - val_loss: 0.1419\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1419 - val_loss: 0.1394\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1394 - val_loss: 0.1369\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1371 - val_loss: 0.1346\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1348 - val_loss: 0.1323\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1327 - val_loss: 0.1302\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1306 - val_loss: 0.1282\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1287 - val_loss: 0.1263\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1268 - val_loss: 0.1245\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1250 - val_loss: 0.1227\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1233 - val_loss: 0.1212\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1218 - val_loss: 0.1195\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1203 - val_loss: 0.1180\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1189 - val_loss: 0.1167\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1176 - val_loss: 0.1155\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1164 - val_loss: 0.1143\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1153 - val_loss: 0.1132\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1143 - val_loss: 0.1122\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1133 - val_loss: 0.1113\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1124 - val_loss: 0.1104\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1116 - val_loss: 0.1096\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1108 - val_loss: 0.1089\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1101 - val_loss: 0.1082\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1094 - val_loss: 0.1075\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1088 - val_loss: 0.1069\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1082 - val_loss: 0.1063\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1076 - val_loss: 0.1058\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1071 - val_loss: 0.1053\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1066 - val_loss: 0.1049\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1062 - val_loss: 0.1044\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1057 - val_loss: 0.1040\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1053 - val_loss: 0.1036\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1050 - val_loss: 0.1032\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1046 - val_loss: 0.1029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff09b9989b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 50 epochs, the autoencoder seems to reach a stable train/test loss value of about 0.11. We can try to visualize the reconstructed inputs and the encoded representations. We will use Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5794335e-06"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_imgs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Sparsity Constraint on encoded representations\n",
    "\n",
    "In the previous example, the representations were only constrained by the size of the hidden layer (32). In such a situation, what typically happens is that the hidden layer is learning an approximation of PCA (principal component analysis). But another way to constrain the representations to be compact is to add a sparsity contraint on the activity of the hidden representations, so fewer units would \"fire\" at a given time. In Keras, this can be done by adding an `activity_regularizer` to our Dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "encoding_dim = 32\n",
    "\n",
    "input_img = Input(shape=(784,))\n",
    "# add a Dense layer with a L1 activity regularizer\n",
    "encoded = Dense(encoding_dim, activation='relu',\n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "\n",
    "encoder = Model(input_img, encoded)\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train this model for 100 epochs with the regulariser included. This will make the model less prone to overfitting; it can also be trained for longer. We'll visualise the results while we're at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.6732 - val_loss: 0.6484\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.6284 - val_loss: 0.6090\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.5916 - val_loss: 0.5749\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.5598 - val_loss: 0.5454\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.5323 - val_loss: 0.5198\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.5084 - val_loss: 0.4975\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.4875 - val_loss: 0.4780\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.4692 - val_loss: 0.4609\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.4531 - val_loss: 0.4457\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.4388 - val_loss: 0.4324\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.4262 - val_loss: 0.4205\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.4150 - val_loss: 0.4098\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.4049 - val_loss: 0.4003\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3959 - val_loss: 0.3918\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3877 - val_loss: 0.3840\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3804 - val_loss: 0.3771\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3737 - val_loss: 0.3707\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3676 - val_loss: 0.3649\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3621 - val_loss: 0.3596\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3570 - val_loss: 0.3548\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3524 - val_loss: 0.3503\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3481 - val_loss: 0.3463\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3442 - val_loss: 0.3425\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3406 - val_loss: 0.3390\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3372 - val_loss: 0.3357\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3341 - val_loss: 0.3327\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3312 - val_loss: 0.3299\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3285 - val_loss: 0.3273\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3259 - val_loss: 0.3249\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.3236 - val_loss: 0.3226\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3213 - val_loss: 0.3204\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3193 - val_loss: 0.3184\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3173 - val_loss: 0.3165\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3155 - val_loss: 0.3147\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3138 - val_loss: 0.3131\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3121 - val_loss: 0.3115\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3106 - val_loss: 0.3100\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3091 - val_loss: 0.3085\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3077 - val_loss: 0.3072\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3064 - val_loss: 0.3059\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3052 - val_loss: 0.3047\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3040 - val_loss: 0.3035\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3029 - val_loss: 0.3024\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3018 - val_loss: 0.3014\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3008 - val_loss: 0.3004\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2998 - val_loss: 0.2994\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2989 - val_loss: 0.2985\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2980 - val_loss: 0.2976\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2971 - val_loss: 0.2968\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2963 - val_loss: 0.2960\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2955 - val_loss: 0.2952\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2948 - val_loss: 0.2945\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2941 - val_loss: 0.2938\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2934 - val_loss: 0.2931\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2927 - val_loss: 0.2925\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2921 - val_loss: 0.2918\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2915 - val_loss: 0.2912\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2909 - val_loss: 0.2906\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2903 - val_loss: 0.2901\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2898 - val_loss: 0.2895\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2892 - val_loss: 0.2890\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2887 - val_loss: 0.2885\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2882 - val_loss: 0.2880\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2877 - val_loss: 0.2875\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2873 - val_loss: 0.2871\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2868 - val_loss: 0.2867\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2864 - val_loss: 0.2862\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2860 - val_loss: 0.2858\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2856 - val_loss: 0.2854\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2852 - val_loss: 0.2850\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2848 - val_loss: 0.2846\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2844 - val_loss: 0.2843\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2841 - val_loss: 0.2839\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.2837 - val_loss: 0.2836\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2834 - val_loss: 0.2832\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2831 - val_loss: 0.2829\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2828 - val_loss: 0.2826\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2825 - val_loss: 0.2823\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2822 - val_loss: 0.2820\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2819 - val_loss: 0.2817\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2816 - val_loss: 0.2814\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2813 - val_loss: 0.2812\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2810 - val_loss: 0.2809\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2808 - val_loss: 0.2806\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2805 - val_loss: 0.2804\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2803 - val_loss: 0.2801\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2800 - val_loss: 0.2799\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2798 - val_loss: 0.2796\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2796 - val_loss: 0.2794\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2793 - val_loss: 0.2792\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2791 - val_loss: 0.2790\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2789 - val_loss: 0.2788\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2787 - val_loss: 0.2785\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2785 - val_loss: 0.2783\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2783 - val_loss: 0.2781\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2781 - val_loss: 0.2779\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.2779 - val_loss: 0.2778\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2777 - val_loss: 0.2776\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2775 - val_loss: 0.2774\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2774 - val_loss: 0.2772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff088015978>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should look similar to the previous model, but the difference should be in the sparsity of the encoded representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5794335e-06"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_imgs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Autoencoders\n",
    "\n",
    "We do not have to limit ourselves to a single layer as encoder or decoder, we could instead use a stack of layers, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3384 - val_loss: 0.2632\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.2573 - val_loss: 0.2500\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.2401 - val_loss: 0.2298\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.2241 - val_loss: 0.2155\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.2068 - val_loss: 0.1995\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1924 - val_loss: 0.1847\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1818 - val_loss: 0.1773\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1750 - val_loss: 0.1702\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1698 - val_loss: 0.1664\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1642 - val_loss: 0.1604\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1593 - val_loss: 0.1571\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1557 - val_loss: 0.1539\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1530 - val_loss: 0.1506\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1503 - val_loss: 0.1489\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1480 - val_loss: 0.1459\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1460 - val_loss: 0.1455\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1442 - val_loss: 0.1426\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1422 - val_loss: 0.1394\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1405 - val_loss: 0.1374\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1387 - val_loss: 0.1367\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1369 - val_loss: 0.1354\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1355 - val_loss: 0.1332\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1340 - val_loss: 0.1316\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1324 - val_loss: 0.1312\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1316 - val_loss: 0.1284\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1302 - val_loss: 0.1298\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1291 - val_loss: 0.1280\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1280 - val_loss: 0.1244\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1269 - val_loss: 0.1235\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1260 - val_loss: 0.1268\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1251 - val_loss: 0.1237\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1242 - val_loss: 0.1222\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1235 - val_loss: 0.1217\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1225 - val_loss: 0.1197\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1217 - val_loss: 0.1197\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1211 - val_loss: 0.1193\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1203 - val_loss: 0.1175\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1195 - val_loss: 0.1193\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1189 - val_loss: 0.1166\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1182 - val_loss: 0.1179\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1176 - val_loss: 0.1155\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1168 - val_loss: 0.1165\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1162 - val_loss: 0.1142\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1157 - val_loss: 0.1133\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1153 - val_loss: 0.1142\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1148 - val_loss: 0.1117\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1143 - val_loss: 0.1133\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1140 - val_loss: 0.1135\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1135 - val_loss: 0.1117\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1130 - val_loss: 0.1108\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1126 - val_loss: 0.1110\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1123 - val_loss: 0.1101\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1120 - val_loss: 0.1116\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1115 - val_loss: 0.1112\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1112 - val_loss: 0.1083\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1109 - val_loss: 0.1085\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1104 - val_loss: 0.1085\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1101 - val_loss: 0.1092\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1097 - val_loss: 0.1091\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1094 - val_loss: 0.1077\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1091 - val_loss: 0.1084\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1088 - val_loss: 0.1077\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1085 - val_loss: 0.1067\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1083 - val_loss: 0.1071\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1078 - val_loss: 0.1075\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1076 - val_loss: 0.1068\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1072 - val_loss: 0.1047\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1071 - val_loss: 0.1043\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1068 - val_loss: 0.1062\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1065 - val_loss: 0.1046\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1062 - val_loss: 0.1051\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1060 - val_loss: 0.1043\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1058 - val_loss: 0.1037\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1055 - val_loss: 0.1066\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1053 - val_loss: 0.1046\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1052 - val_loss: 0.1048\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1049 - val_loss: 0.1074\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1047 - val_loss: 0.1043\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1046 - val_loss: 0.1027\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1044 - val_loss: 0.1035\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1041 - val_loss: 0.1030\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1040 - val_loss: 0.1027\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1037 - val_loss: 0.1032\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1035 - val_loss: 0.1015\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1034 - val_loss: 0.1015\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1030 - val_loss: 0.1026\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1027 - val_loss: 0.1023\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1026 - val_loss: 0.1019\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.1023 - val_loss: 0.1001\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1021 - val_loss: 0.1008\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1020 - val_loss: 0.1019\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1017 - val_loss: 0.1010\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1016 - val_loss: 0.1011\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1016 - val_loss: 0.1008\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1013 - val_loss: 0.0999\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1012 - val_loss: 0.1014\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1010 - val_loss: 0.1003\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1008 - val_loss: 0.1002\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1006 - val_loss: 0.0986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff07801bac8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(128, activation='relu')(input_img)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(784, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 100 epochs, it reaches a train and test loss of ~0.097, a bit better than our previous models. Our reconstructed digits look a bit better too:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Autoencoders\n",
    "\n",
    "Since our inputs are images, it makes sense to use convolutional neural networks (convnets) as encoders and decoders. In practical settings, autoencoders applied to images are always convolutional autoencoders --they simply perform much better.\n",
    "\n",
    "Let's implement one. The encoder will consist in a stack of Conv2D and MaxPooling2D layers (max pooling being used for spatial down-sampling), while the decoder will consist in a stack of Conv2D and UpSampling2D layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2143 - val_loss: 0.1652\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1562 - val_loss: 0.1416\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1402 - val_loss: 0.1420\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1322 - val_loss: 0.1300\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.1278 - val_loss: 0.1266\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1239 - val_loss: 0.1257\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.1212 - val_loss: 0.1209\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1189 - val_loss: 0.1134\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.1168 - val_loss: 0.1128\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.1153 - val_loss: 0.1142\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1139 - val_loss: 0.1132\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1125 - val_loss: 0.1111\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.1116 - val_loss: 0.1092\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1107 - val_loss: 0.1086\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1099 - val_loss: 0.1050\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1090 - val_loss: 0.1098\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.1083 - val_loss: 0.1056\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.1075 - val_loss: 0.1060\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.1066 - val_loss: 0.1062\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1060 - val_loss: 0.1095\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1053 - val_loss: 0.1059\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.1048 - val_loss: 0.1051\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1045 - val_loss: 0.1034\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.1038 - val_loss: 0.1002\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1034 - val_loss: 0.1018\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1029 - val_loss: 0.0996\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1028 - val_loss: 0.1039\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1023 - val_loss: 0.1026\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.1018 - val_loss: 0.1014\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.1016 - val_loss: 0.0988\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.1013 - val_loss: 0.1009\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.1009 - val_loss: 0.1014\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.1006 - val_loss: 0.1014\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.1005 - val_loss: 0.0970\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1003 - val_loss: 0.1019\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1000 - val_loss: 0.0978\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.0997 - val_loss: 0.0956\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.0996 - val_loss: 0.0977\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.0993 - val_loss: 0.0989\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.0993 - val_loss: 0.0979\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0990 - val_loss: 0.0963\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0987 - val_loss: 0.0996\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0986 - val_loss: 0.0971\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0982 - val_loss: 0.0992\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0982 - val_loss: 0.1027\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0979 - val_loss: 0.0956\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.0977 - val_loss: 0.0973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fefff42ef60>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
